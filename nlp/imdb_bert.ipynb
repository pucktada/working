{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f905847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47be875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/pucktada/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54c49f8f03c476fa8b036c10c0f458b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "imdb = load_dataset(\"imdb\")\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e76a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac717e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079af01361674885bfe8c49facddafce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628bfa277d9344608a44a2c52999f30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ad25ae6555406f85d44a84a53ac667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dc3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data['train'].map(preprocessor), # dataset\n",
    "                          sampler=RandomSampler(self.data['train']), # random sampler\n",
    "                          batch_size=batch_size)\n",
    "\n",
    "    #def val_dataloader(self):\n",
    "    #    return DataLoader(imdb_datadict['val'].map(preprocessor),\n",
    "    #                      sampler=SequentialSampler(imdb_datadict['val']),\n",
    "    #                      batch_size=batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data['test'].map(preprocessor),\n",
    "                          sampler=SequentialSampler(self.data['test']),\n",
    "                          batch_size=batch_size)\n",
    "    \n",
    "    #def predict_dataloader(self):\n",
    "    #    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd2198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "        self.model = model\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay_rate\": 0.01\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay_rate\": 0.0\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "        \n",
    "        loss, logits = self.model(input_ids, \n",
    "                                  token_type_ids=token_type_ids, \n",
    "                                  attention_mask=attention_mask, \n",
    "                                  labels=labels)\n",
    "        return loss, logits\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logits = self._common_step(batch, batch_idx)\n",
    "        output = OrderedDict({ \"loss\": loss })\n",
    "\n",
    "        #tqdm_dict = {\"train_loss\": loss}\n",
    "        #output = OrderedDict({ \"loss\": loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict })\n",
    "        return output\n",
    "\n",
    "    #def validation_step(self, batch, batch_idx):\n",
    "    #    loss, logits = self._common_step(batch, batch_idx)        \n",
    "    #    labels_hat = torch.argmax(logits, dim=1)\n",
    "    #    correct_count = torch.sum(labels == labels_hat)\n",
    "    #    if self.on_gpu:\n",
    "    #        correct_count = correct_count.cuda(loss.device.index)\n",
    "    #    output = OrderedDict({ \"val_loss\": loss, \"correct_count\": correct_count, \"batch_size\": len(labels) })\n",
    "    #    return output\n",
    "\n",
    "    #def validation_end(self, outputs):\n",
    "    #    val_acc = sum([out[\"correct_count\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
    "    #    val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
    "    #    tqdm_dict = { \"val_loss\": val_loss, \"val_acc\": val_acc, }\n",
    "    #    result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n",
    "    #    return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, logits = self._common_step(batch, batch_idx)\n",
    "\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "        correct_count = torch.sum(labels == labels_hat)\n",
    "        \n",
    "        #if self.on_gpu:\n",
    "        #    correct_count = correct_count.cuda(loss.device.index)\n",
    "        output = OrderedDict({ \"test_loss\": loss, \"correct_count\": correct_count, \"batch_size\": len(labels) })\n",
    "        return output\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        test_acc  = sum([out[\"correct_count\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
    "        test_loss = sum([out[\"test_loss\"] for out in outputs]) / len(outputs)\n",
    "        result = {}\n",
    "        \n",
    "        #tqdm_dict = { \"test_loss\": test_loss, \"test_acc\": test_acc, }\n",
    "        #result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "        \n",
    "        return result\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696c921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
